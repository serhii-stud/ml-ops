import pandas as pd
import requests
import time
from sklearn.metrics import accuracy_score, classification_report

# --- CONFIGURATION ---
API_URL = "http://0.0.0.0:8000/predict"
TEST_DATA_PATH = "data/raw/test.csv"  # –£–∫–∞–∂–∏ –ø—É—Ç—å –∫ —Ç–≤–æ–µ–º—É —Ç–µ—Å—Ç–æ–≤–æ–º—É —Ñ–∞–π–ª—É
# –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ test.csv –Ω–µ—Ç, –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–∞ –∫—É—Å–æ—á–∫–µ train.csv:
# TEST_DATA_PATH = "data/raw/train.csv"

LIMIT_SAMPLES = 200  # –ù–µ –±—É–¥–µ–º –∂–¥–∞—Ç—å –≤–µ—á–Ω–æ—Å—Ç—å, –ø—Ä–æ–≤–µ—Ä–∏–º –Ω–∞ 200 –ø—Ä–∏–º–µ—Ä–∞—Ö


def benchmark():
    print(f"üöÄ Starting Benchmark using {TEST_DATA_PATH}...")

    try:
        # –ß–∏—Ç–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        df = pd.read_csv(TEST_DATA_PATH)

        # –ï—Å–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –æ–≥—Ä–æ–º–Ω—ã–π, –±–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É
        if len(df) > LIMIT_SAMPLES:
            print(f"‚úÇÔ∏è Sampling {LIMIT_SAMPLES} random rows from {len(df)} total.")
            df = df.sample(LIMIT_SAMPLES, random_state=42)

        y_true = []
        y_pred = []
        errors = []

        print(f"‚è≥ Sending {len(df)} requests...")

        start_global = time.time()

        for index, row in df.iterrows():
            text = row['text']
            true_label = row['category']

            try:
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
                response = requests.post(API_URL, json={"text": text})

                if response.status_code == 200:
                    pred_label = response.json()['category']

                    y_true.append(true_label)
                    y_pred.append(pred_label)

                    # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞, –∑–∞–ø–æ–º–∏–Ω–∞–µ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                    if pred_label != true_label:
                        errors.append({
                            "text": text,
                            "true": true_label,
                            "pred": pred_label
                        })
                else:
                    print(f"‚ö†Ô∏è API Error: {response.status_code}")

            except Exception as e:
                print(f"üö® Connection Error: {e}")
                break

            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –Ω–µ –Ω—É–∂–Ω–∞, –º—ã —Ö–æ—Ç–∏–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–∫–æ—Ä–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã —Ç–æ–∂–µ,
            # –Ω–æ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫–µ—Ä–∞ –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –º–∏–∫—Ä–æ-–ø–∞—É–∑—É
            # time.sleep(0.01)

        total_time = time.time() - start_global

        # --- –û–¢–ß–ï–¢ ---
        print("\n" + "=" * 40)
        print("üìä BENCHMARK RESULTS")
        print("=" * 40)

        acc = accuracy_score(y_true, y_pred)
        print(f"‚úÖ Accuracy: {acc:.2%}")
        print(f"‚è±Ô∏è Avg Latency: {total_time / len(df):.4f} sec/req")

        print("\n‚ùå Top 5 Mistakes:")
        for err in errors[:5]:
            print(f"   Input: '{err['text'][:50]}...'")
            print(f"   Expected: {err['true']}")
            print(f"   Got:      {err['pred']}")
            print("-" * 20)

    except FileNotFoundError:
        print(f"‚ùå Error: File {TEST_DATA_PATH} not found.")
        print("Please check the path or put a CSV file with 'text' and 'category' columns.")


if __name__ == "__main__":
    benchmark()